# ML-learning
> Samples of some ML algorithms and visualisations

## STS (Semantic Textual Similarity)

- cleaning data;
- preprocessing data for transformers;
- creating embeddings with paraphrase-multilingual-MiniLM-L12-v2 Transformer;
- creating embeddings with universal-sentence-encoder-multilingual Transformer;
- comparing embeddings Cosine similarity.

## Efficent Apriori and FP-Growth - Frequent itemset mining algorithms

- prepocessing data;
- creating frequent itemset and rules;
- visualising Efficent Apriori rules with PyARMViz library;
- visualize and output FP-tree with Graphviz;
- testing the running time of the algorithms.

## KNN, SVM, RF algorithms + visualising with t-SNE and UMAP

- prepocessing data;
- normalizing data with StandardScaler;
- balancing train data with SMOTE;
- GridSearch hyperparameters and comparing results;
- visualising models with t-SNE and UMAP;
- checking CatBoost algorithm with the same data;
- looking feature importance (FI) with CatBoost and SHAP.

## Rosenbrock function Optimization (Newton, GD, PSO, GA)

- plotting Ronsenbrock function 3D and 2D;
- implementing Newton's method;
- implementing Gradient Descent;
- implementing Particle Swarm Optimization (PSO) algorithm;
- implementing Genetic Algorithm algorithm (GA);
- looking at Law of large numbers and Central limit theorem;
- comparing GA, PSO and Gradient Descent;
- animating GD and PSO algorithms.

## K-means, DBSCAN and FCM clusterizations

- creating clusters with 10 centers;
- K-Means clusterization with 2-10 clusters;
- DBSCAN clusterization with different eps and min_samples;
- Fuzzy-C-Means clusterization with 2-10 clusters;
- comparing best clusterizations.

## Recurrent Neural Networks (Time Series) - LTSM, GRU, Simple_RNN

- prepocessing data;
- normalizing data with MinMax scaler;
- generating time series;
- implementing LSTM model;
- implementing GRU model;
- implementing SimpleRNN model;
- comparing results on CPU and GPU learning (Google Collab).
